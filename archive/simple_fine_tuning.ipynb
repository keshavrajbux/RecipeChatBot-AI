{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzRa3xvznvbN",
        "outputId": "4391684a-cdb4-457a-a0f7-2ca139886ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_folder = Path('/content/drive/MyDrive/')\n",
        "data_folder = Path('/content')\n",
        "\n",
        "!pip install pytorch-lightning==2.0.9 -qq\n",
        "\n",
        "!pip install torchmetrics -U -qq\n",
        "!pip install fastdownload -U -qq\n",
        "!pip install fastai -U -qq\n",
        "!pip install wandb -U -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8CUBpUGK2-kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad283e3-a261-497a-b65d-8f3981529093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lk5xPXtk3XEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bedf9a-9a2d-4717-cffb-8326e39beaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3PpB1SIKm_O-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FrLP7PXmZi4W"
      },
      "outputs": [],
      "source": [
        "output_dir = '/content/drive/MyDrive/FoodBuddy/Model2'\n",
        "test_file = \"/content/drive/MyDrive/FoodBuddy/unsupervised_test.txt\"\n",
        "def calculate_perplexity(logits):\n",
        "    return np.exp(logits.mean())\n",
        "\n",
        "def calculate_burstiness(text):\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    burstiness = unique_words / word_count\n",
        "    return burstiness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omfBqDk3aE1S"
      },
      "outputs": [],
      "source": [
        "def fine_tune_gpt2(model_name, train_file,val_file, output_dir):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Adjust block_size for training and evaluation datasets\n",
        "    block_size = 64  # Experiment with smaller values for faster processing\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file,\n",
        "        block_size=block_size\n",
        "    )\n",
        "    eval_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=val_file,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "    # Use smaller batch sizes for faster training\n",
        "    batch_size = 4  # Experiment with smaller values for faster processing\n",
        "\n",
        "    # Data collator for language modeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,\n",
        "        save_steps=10000,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Trainer setup\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate perplexity\n",
        "    eval_dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
        "    model.eval()\n",
        "    perplexity = []\n",
        "    for batch in eval_dataloader:\n",
        "        inputs, labels = batch[\"input_ids\"], batch[\"labels\"]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            perplexity.append(calculate_perplexity(logits))\n",
        "\n",
        "    avg_perplexity = np.mean(perplexity)\n",
        "    print(f\"Average Perplexity: {avg_perplexity}\")\n",
        "\n",
        "    # Generate text for burstiness calculation\n",
        "    generated_text = model.generate(\n",
        "        max_length=500,  # Adjust length for faster generation\n",
        "        temperature=0.7,\n",
        "        top_k=100,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        num_return_sequences=1\n",
        "    )[0]\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_text, skip_special_tokens=True)\n",
        "    burstiness = calculate_burstiness(generated_text)\n",
        "    print(f\"Burstiness: {burstiness}\")\n",
        "\n",
        "    # Save the fine-tuned model and tokenizer\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_gpt2(model_name, train_file,val_file, output_dir):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Adjust block_size for training and evaluation datasets\n",
        "    block_size = 64  # Experiment with smaller values for faster processing\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file,\n",
        "        block_size=block_size\n",
        "    )\n",
        "    eval_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=val_file,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "    # Use smaller batch sizes for faster training\n",
        "    batch_size = 4  # Experiment with smaller values for faster processing\n",
        "\n",
        "    # Data collator for language modeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    save_steps=10000,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    learning_rate=5e-6, # Initial learning rate\n",
        "    max_grad_norm=1.0, # Gradient clipping\n",
        "    weight_decay=0.04, # Weight decay\n",
        "    logging_dir=None, # Directory for storing logs\n",
        "    logging_steps=100, # Interval for logging\n",
        "    max_steps=500, # Total number of steps for learning rate finder\n",
        "    find_unused_parameters=True, # Search for unused parameters\n",
        "    )\n",
        "\n",
        "    # Trainer setup\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate perplexity\n",
        "    eval_dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
        "    model.eval()\n",
        "    perplexity = []\n",
        "    for batch in eval_dataloader:\n",
        "        inputs, labels = batch[\"input_ids\"], batch[\"labels\"]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            perplexity.append(calculate_perplexity(logits))\n",
        "\n",
        "    avg_perplexity = np.mean(perplexity)\n",
        "    print(f\"Average Perplexity: {avg_perplexity}\")\n",
        "\n",
        "    # Generate text for burstiness calculation\n",
        "    generated_text = model.generate(\n",
        "        max_length=500,  # Adjust length for faster generation\n",
        "        temperature=0.7,\n",
        "        top_k=100,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        num_return_sequences=1\n",
        "    )[0]\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_text, skip_special_tokens=True)\n",
        "    burstiness = calculate_burstiness(generated_text)\n",
        "    print(f\"Burstiness: {burstiness}\")\n",
        "\n",
        "    # Save the fine-tuned model and tokenizer\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "mQqpgJkUIjPm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcwEEk_knZIf",
        "outputId": "1c33aaa9-c7ae-4d44-eba3-dfa82be7a5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "text_file='/content/drive/MyDrive/FoodBuddy/unsupervised_train.txt'\n",
        "fine_tune_gpt2(\"mbien/recipenlg\",text_file,test_file, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Load loss values from file\n",
        "losses = np.load('losses.npy')\n",
        "\n",
        "# Load learning rate values from file\n",
        "lrs = np.load('lrs.npy')\n",
        "\n",
        "# Plot loss as a function of learning rate\n",
        "plt.plot(lrs, losses)\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning Rate Finder')\n",
        "plt.xscale('log')\n",
        "plt.ylim(0, max(losses))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-G9Oz9q2JDWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GxWMghSBNkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54fab55-2836-4ac2-9e28-25482670bf63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IPE_ START>  chocolate chips  butterscotch chips  condensed milk  pecans  coconut  vanilla   1 small pkg. chocolate chips  1 small pkg. butterscotch chips  1 can sweetened condensed milk  1 c. chopped pecans  1 c. coconut  1 tsp. vanilla   Melt chips and milk together.  Add pecans\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Assuming you have specified your fine-tuned model directory\n",
        "model_dir = output_dir\n",
        "\n",
        "# Load tokenizer and model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "\n",
        "# Generate text using the fine-tuned model\n",
        "generated_text = model.generate(\n",
        "    max_length=100,  # Adjust the length of generated text\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from flask import Flask, render_template, request, jsonify"
      ],
      "metadata": {
        "id": "x8C5utzUVXPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install pyngrok"
      ],
      "metadata": {
        "id": "y9p0WuBphmdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install Flask Jinja2"
      ],
      "metadata": {
        "id": "LUf6xUKkU5iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install flask==0.12.2\n",
        "# !pip install flask-ngrok\n",
        "# # print flask version\n",
        "# from flask import Flask\n",
        "# from flask_ngrok import run_with_ngrok\n"
      ],
      "metadata": {
        "id": "QQ7DRAnEDWTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "da694340-773c-47ce-eaaf-a4845ac75cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask==0.12.2\n",
            "  Downloading Flask-0.12.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.10/dist-packages (from flask==0.12.2) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.10/dist-packages (from flask==0.12.2) (3.0.1)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask==0.12.2) (8.1.7)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.10/dist-packages (from flask==0.12.2) (2.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.4->flask==0.12.2) (2.1.3)\n",
            "Installing collected packages: flask\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 2.2.5\n",
            "    Uninstalling Flask-2.2.5:\n",
            "      Successfully uninstalled Flask-2.2.5\n",
            "Successfully installed flask-0.12.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flask"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (0.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.1)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.4->Flask>=0.8->flask-ngrok) (2.1.3)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ngrok authtoken '2Yobuvo9E68KBLZ9ChOd3WckM4b_7n6F8JuKQRXW7GbGAkj2B'"
      ],
      "metadata": {
        "id": "lVs2g4lrCkjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39127438-62f8-455f-f2f1-e0202585ecb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import threading\n",
        "# from flask import Flask, render_template, request, jsonify"
      ],
      "metadata": {
        "id": "qc9nW05siOr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install Flask requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4MXisMOH71c",
        "outputId": "13b39367-9c3e-4861-dfb1-eb841f145cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.1)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.4->Flask) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app = Flask(__name__)\n",
        "# run_with_ngrok(app)\n",
        "\n",
        "# @app.route('/')\n",
        "# def index():\n",
        "#     # Render the main interface where users can input their cravings or ingredients.\n",
        "#     return render_template('/content/drive/MyDrive/FoodBuddy/index.html')\n",
        "# @app.route('/get_recipe', methods=['POST'])\n",
        "# def get_recipe():\n",
        "#     # Load tokenizer and model\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "#     model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "#     try:\n",
        "#         user_input = request.json['user_input']\n",
        "#         # Tokenize the user input\n",
        "#         input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
        "#         # Generate a response from the model\n",
        "#         gpt_response = model.generate(\n",
        "#             input_ids,\n",
        "#             max_length=100,\n",
        "#             num_beams=5,\n",
        "#             no_repeat_ngram_size=2,\n",
        "#             top_k=10,\n",
        "#             top_p=0.95,\n",
        "#             temperature=0.7\n",
        "#         )\n",
        "#         # Decode the generated response\n",
        "#         decoded_response = tokenizer.decode(gpt_response[0], skip_special_tokens=True)\n",
        "#         return jsonify({'response': decoded_response, 'status': 'success'})\n",
        "#     except Exception as e:\n",
        "#         # In case of an error, return an error message\n",
        "#         return jsonify({'response': str(e), 'status': 'error'})\n",
        "\n",
        "# app.run()"
      ],
      "metadata": {
        "id": "IC9lApxys8e-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}